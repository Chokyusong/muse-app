# heart_aggregate.py
import io, re, csv, zipfile, unicodedata
import numpy as np
import pandas as pd
import streamlit as st
from openpyxl import load_workbook
from openpyxl.utils import get_column_letter
from datetime import datetime

# ===== ì—¬ê¸°ë¶€í„°ëŠ” ê¸°ì¡´ app.py â€œë‚´ìš©â€ì„ í•¨ìˆ˜ë¡œ ê°ìŒˆ =====
def show():
    # â†“â†“â†“ ë‹¹ì‹ ì´ ì´ì „ì— ì¤€ app.py ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì˜®ê¸°ë˜,
    #     st.set_page_config(...) í•œ ì¤„ë§Œ ë¹¼ê³  ëª¨ë‘ í¬í•¨ì‹œí‚¤ë©´ ë©ë‹ˆë‹¤.
    #     (ì•„ë˜ëŠ” í•µì‹¬ë§Œ ê·¸ëŒ€ë¡œ ê°€ì ¸ë‹¤ ë‘” ë²„ì „)

    st.subheader("BJë³„ í•˜íŠ¸ ì •ë¦¬ ìë™í™”")
    st.caption("ë‹¨ì¼ íŒŒì¼ â†’ ê´€ë¦¬ììš©/BJìš© ZIP (í•©ì‚°), ì—¬ëŸ¬ íŒŒì¼ â†’ ì´í•©ì‚° ì—‘ì…€ (ìš”ì•½ + ì°¸ì—¬BJë³„ ì‹œíŠ¸)")

    # -------------------- helpers --------------------
    def visual_len(val) -> int:
        s = str(val) if val is not None else ""
        w = 0
        for ch in s:
            if unicodedata.east_asian_width(ch) in ("F", "W", "A"):
                w += 2
            elif ord(ch) >= 0x1F300:
                w += 2
            else:
                w += 1
        return w

    def autosize_columns(wb, min_w=12, max_w=80, pad=2):
        for ws in wb.worksheets:
            for col in ws.columns:
                letter = get_column_letter(col[0].column)
                max_width = 0
                for cell in col:
                    if cell.value is not None:
                        max_width = max(max_width, visual_len(cell.value))
                ws.column_dimensions[letter].width = max(min_w, min(max_width + pad, max_w))

    def sanitize(name: str) -> str:
        return re.sub(r'[\\/*?:\[\]]', "_", str(name))[:31] or "BJ"

    def normalize_nick(nick: str) -> str:
        if not isinstance(nick, str):
            return ""
        nick = re.sub(r'^\[.*?\]', '', nick)
        nick = re.sub(r'\(.*?\)', '', nick)
        return nick.strip()

    def normalize_bj(name: str) -> str:
        if not isinstance(name, str):
            return ""
        return re.sub(r'^\[.*?\]', '', name).strip()

    @st.cache_data(show_spinner=False, persist=False, ttl=0, max_entries=10)
    def read_any_table(uploaded_file, sheet: str | int | None):
        name = (uploaded_file.name or "").lower()
        if name.endswith(".xlsx"):
            return pd.read_excel(uploaded_file, sheet_name=(sheet if str(sheet).strip() else 0))
        raw = uploaded_file.read(); uploaded_file.seek(0)
        for enc in ["utf-8", "utf-8-sig", "cp949", "euc-kr"]:
            try:
                text = raw.decode(enc)
                try:
                    dialect = csv.Sniffer().sniff(text[:4000], delimiters=[",", "\t", ";", "|"])
                    sep = dialect.delimiter
                except Exception:
                    sep = ","
                return pd.read_csv(io.StringIO(text), sep=sep)
            except Exception:
                continue
        raise ValueError("CSV ì¸ì½”ë”©/êµ¬ë¶„ì í•´ì„ ì‹¤íŒ¨")

    # ---------------- ë‹¨ì¼ íŒŒì¼ (ê´€ë¦¬ììš©/BJìš© ZIP) ----------------
    def preprocess(df: pd.DataFrame) -> pd.DataFrame:
        df.columns = [str(c).strip() for c in df.columns]
        col_bj    = next((c for c in df.columns if c == "ì°¸ì—¬BJ"), None)
        col_heart = next((c for c in df.columns if c == "í›„ì›í•˜íŠ¸"), None)
        col_mix   = next((c for c in df.columns if c == "í›„ì› ì•„ì´ë””(ë‹‰ë„¤ì„)"), None)
        if not (col_bj and col_heart and col_mix):
            raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: ì°¸ì—¬BJ / í›„ì›í•˜íŠ¸ / í›„ì› ì•„ì´ë””(ë‹‰ë„¤ì„)")

        df[col_bj] = df[col_bj].astype(str).str.strip()
        df[col_heart] = df[col_heart].astype(str).str.replace(",", "", regex=False)
        df[col_heart] = pd.to_numeric(df[col_heart], errors="coerce").fillna(0).astype(int)
        df[col_mix] = df[col_mix].astype(str).str.strip()

        sp = df[col_mix].str.extract(r'^\s*(?P<ID>[^()]+?)(?:\((?P<NICK>.*)\))?\s*$')
        df["ID"] = sp["ID"].fillna("").str.replace("ï¼ ","@",regex=False).str.strip()
        df["ë‹‰ë„¤ì„"] = sp["NICK"].fillna("").apply(normalize_nick)

        base = (
            df.groupby([col_bj, "ID", "ë‹‰ë„¤ì„"], as_index=False)[col_heart]
              .sum()
              .rename(columns={col_bj:"ì°¸ì—¬BJ", col_heart:"í›„ì›í•˜íŠ¸"})
        )
        return base

    def _xlsx_bytes_from_df(writer_fn) -> bytes:
        bio = io.BytesIO()
        with pd.ExcelWriter(bio, engine="openpyxl") as w:
            writer_fn(w)
        bio.seek(0)
        wb = load_workbook(bio)
        autosize_columns(wb)
        out = io.BytesIO(); wb.save(out); out.seek(0)
        return out.getvalue()

    def make_bj_excel(bj_name: str, sub_df: pd.DataFrame, admin: bool) -> bytes:
        sub = sub_df.copy()
        sub["is_aff"] = sub["ID"].str.contains("@")
        gen = sub[~sub["is_aff"]].sort_values("í›„ì›í•˜íŠ¸", ascending=False)[["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸"]].copy()
        aff = sub[ sub["is_aff"]].sort_values("í›„ì›í•˜íŠ¸", ascending=False)[["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸"]].copy()
        gsum, asum = int(gen["í›„ì›í•˜íŠ¸"].sum()), int(aff["í›„ì›í•˜íŠ¸"].sum())
        total = gsum + asum
        sheet = sanitize(bj_name)

        def _write(w):
            if admin:
                row1 = pd.DataFrame([[ "", bj_name, total, "", "" ]],
                                    columns=["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸","êµ¬ë¶„","í•©ê³„"])
            else:
                row1 = pd.DataFrame([[ "", bj_name, total ]],
                                    columns=["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸"])
            row1.to_excel(w, sheet_name=sheet, index=False, header=False, startrow=0)

            if admin:
                pd.DataFrame(columns=["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸","êµ¬ë¶„","í•©ê³„"]).to_excel(
                    w, sheet_name=sheet, index=False, startrow=1)
            else:
                pd.DataFrame(columns=["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸"]).to_excel(
                    w, sheet_name=sheet, index=False, startrow=1)

            row = 2
            if not gen.empty:
                blk = gen.copy()
                if admin:
                    blk["êµ¬ë¶„"], blk["í•©ê³„"] = "", ""
                    blk.iloc[0, blk.columns.get_loc("êµ¬ë¶„")] = "ì¼ë°˜í•˜íŠ¸"
                    blk.iloc[0, blk.columns.get_loc("í•©ê³„")] = gsum
                blk.to_excel(w, sheet_name=sheet, index=False, header=False, startrow=row)
                row += len(blk)
            if not aff.empty:
                blk = aff.copy()
                if admin:
                    blk["êµ¬ë¶„"], blk["í•©ê³„"] = "", ""
                    blk.iloc[0, blk.columns.get_loc("êµ¬ë¶„")] = "ì œíœ´í•˜íŠ¸"
                    blk.iloc[0, blk.columns.get_loc("í•©ê³„")] = asum
                blk.to_excel(w, sheet_name=sheet, index=False, header=False, startrow=row)

        return _xlsx_bytes_from_df(_write)

    def build_file_sets(base: pd.DataFrame):
        summary = base.groupby("ì°¸ì—¬BJ", as_index=False)["í›„ì›í•˜íŠ¸"].sum().sort_values("í›„ì›í•˜íŠ¸", ascending=False)

        def make_summary_bytes() -> bytes:
            return _xlsx_bytes_from_df(lambda w: summary.to_excel(w, sheet_name="ìš”ì•½", index=False))

        def pack_zip(files: dict[str, bytes]) -> bytes:
            zbio = io.BytesIO()
            with zipfile.ZipFile(zbio, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                for fname, data in files.items():
                    zf.writestr(fname, data)
            zbio.seek(0); return zbio.getvalue()

        admin_files, bj_files = {"ìš”ì•½.xlsx": make_summary_bytes()}, {"ìš”ì•½.xlsx": make_summary_bytes()}
        for bj in summary["ì°¸ì—¬BJ"]:
            sub = base[base["ì°¸ì—¬BJ"] == bj][["ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸"]]
            admin_files[f"{sanitize(bj)}.xlsx"] = make_bj_excel(str(bj), sub, admin=True)
            bj_files[f"{sanitize(bj)}.xlsx"] = make_bj_excel(str(bj), sub, admin=False)
        return (admin_files, pack_zip(admin_files)), (bj_files, pack_zip(bj_files))

    # ---------------- ì—¬ëŸ¬ íŒŒì¼ (ì´í•©ì‚° ì—‘ì…€) ----------------
    def extract_date_from_name(name: str) -> str:
        s = name.lower()
        m = re.search(r'(20\d{2})[.\-_](\d{1,2})[.\-_](\d{1,2})', s)
        if m: return f"{int(m[1]):04d}-{int(m[2]):02d}-{int(m[3]):02d}"
        m = re.search(r'(20\d{2})(\d{2})(\d{2})', s)
        if m: return f"{int(m[1]):04d}-{int(m[2]):02d}-{int(m[3]):02d}"
        m = re.search(r'(\d{2})[.\-_](\d{1,2})[.\-_](\d{1,2})', s)
        if m: return f"{2000+int(m[1]):04d}-{int(m[2]):02d}-{int(m[3]):02d}"
        m = re.search(r'(\d{1,2})(\d{2})', s)
        if m and len(m.group(0)) == 4:
            y = datetime.now().year
            return f"{y:04d}-{int(m[1]):02d}-{int(m[2]):02d}"
        return datetime.now().strftime("%Y-%m-%d")

    def build_master_excel_bytes(merged_df, df_daily, df_total) -> bytes:
        bio = io.BytesIO()
        with pd.ExcelWriter(bio, engine="openpyxl") as w:
            df_daily.to_excel(w, index=False, sheet_name="ìš”ì•½_ì¼ë³„")
            df_total.to_excel(w, index=False, sheet_name="ìš”ì•½_ì°¸ì—¬BJ_ì´ê³„")
            merged_df = merged_df.copy()
            merged_df["ì°¸ì—¬BJ_ì •ê·œí™”"] = merged_df["ì°¸ì—¬BJ"].apply(normalize_bj)
            sort_cols = [c for c in ["ë‚ ì§œ","í›„ì›ì‹œê°„"] if c in merged_df.columns]
            if sort_cols:
                merged_df = merged_df.sort_values(sort_cols)
            for bj, sub in merged_df.groupby("ì°¸ì—¬BJ_ì •ê·œí™”"):
                gsum = int(sub.loc[sub["êµ¬ë¶„"]=="ì¼ë°˜í•˜íŠ¸","í›„ì›í•˜íŠ¸"].sum())
                asum = int(sub.loc[sub["êµ¬ë¶„"]=="ì œíœ´í•˜íŠ¸","í›„ì›í•˜íŠ¸"].sum())
                tsum = gsum + asum
                top = pd.DataFrame([[f"ì´ ì¼ë°˜í•˜íŠ¸={gsum}", f"ì´ ì œíœ´í•˜íŠ¸={asum}", f"ì´í•©={tsum}"]])
                cols = ["ë‚ ì§œ","í›„ì›ì‹œê°„","ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸","êµ¬ë¶„"]
                exist_cols = [c for c in cols if c in sub.columns]
                out = sub[exist_cols].reset_index(drop=True)
                sheet = sanitize(bj)
                top.to_excel(w, index=False, header=False, sheet_name=sheet, startrow=0)
                out.to_excel(w, index=False, sheet_name=sheet, startrow=2)
        bio.seek(0); wb = load_workbook(bio); autosize_columns(wb)
        out = io.BytesIO(); wb.save(out); out.seek(0)
        return out.getvalue()

    # ================== UI ==================
    uploaded = st.file_uploader("ë‹¨ì¼ CSV/XLSX ì—…ë¡œë“œ (ê´€ë¦¬ììš©/BJìš© ZIP ìƒì„± â€” í•©ì‚°)", type=["csv", "xlsx"])
    sheet_name = st.text_input("ì‹œíŠ¸ ì´ë¦„ (ì—‘ì…€ì¼ ë•Œë§Œ)", value="")

    if uploaded:
        try:
            df_in = read_any_table(uploaded, sheet_name if uploaded.name.lower().endswith(".xlsx") else None)
            base = preprocess(df_in)
            (admin_files, admin_zip), (bj_files, bj_zip) = build_file_sets(base)
            left, right = st.columns(2, gap="large")
            with left:
                st.subheader("ê´€ë¦¬ììš© (í•©ì‚°, êµ¬ë¶„/í•©ê³„ í¬í•¨)")
                st.download_button("ğŸ“¦ ê´€ë¦¬ììš© ZIP ë‹¤ìš´ë¡œë“œ", data=admin_zip,
                                   file_name="BJë³„_ê´€ë¦¬ììš©.zip", mime="application/zip",
                                   use_container_width=True, key="zip-admin")
            with right:
                st.subheader("BJìš© (í•©ì‚°, ì‹¬í”Œë²„ì „)")
                st.download_button("ğŸ“¦ BJìš© ZIP ë‹¤ìš´ë¡œë“œ", data=bj_zip,
                                   file_name="BJë³„_BJìš©.zip", mime="application/zip",
                                   use_container_width=True, key="zip-bj")
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")

    st.header("ì—¬ëŸ¬ íŒŒì¼ í•©ì‚° (ì´í•©ì‚° ì—‘ì…€ ìƒì„±)")
    multi = st.file_uploader("ì—¬ëŸ¬ CSV/XLSX ì—…ë¡œë“œ", type=["csv","xlsx"], accept_multiple_files=True)

    if multi:
        all_rows = []
        for uf in multi:
            try:
                date_str = extract_date_from_name(uf.name)
                df_in = read_any_table(uf, None)
                mix_col = "í›„ì› ì•„ì´ë””(ë‹‰ë„¤ì„)"
                if mix_col not in df_in.columns:
                    raise ValueError(f"{uf.name}: '{mix_col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                sp = df_in[mix_col].astype(str).str.extract(r'^\s*(?P<ID>[^()]+?)(?:\((?P<NICK>.*)\))?\s*$')
                df_in["ID"] = sp["ID"].fillna("").str.replace("ï¼ ","@",regex=False).str.strip()
                df_in["ë‹‰ë„¤ì„"] = sp["NICK"].fillna("").apply(normalize_nick)
                df_in["êµ¬ë¶„"] = np.where(df_in["ID"].str.contains("@"), "ì œíœ´í•˜íŠ¸", "ì¼ë°˜í•˜íŠ¸")
                df_in["ë‚ ì§œ"] = date_str
                cols = ["ë‚ ì§œ","í›„ì›ì‹œê°„","ì°¸ì—¬BJ","ID","ë‹‰ë„¤ì„","í›„ì›í•˜íŠ¸","êµ¬ë¶„"]
                exist_cols = [c for c in cols if c in df_in.columns]
                all_rows.append(df_in[exist_cols])
            except Exception as e:
                st.warning(f"{uf.name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        if all_rows:
            merged = pd.concat(all_rows, ignore_index=True)
            need_cols = {"ë‚ ì§œ","ì°¸ì—¬BJ","êµ¬ë¶„","í›„ì›í•˜íŠ¸"}
            if not need_cols.issubset(set(merged.columns)):
                st.error("í•„ìˆ˜ ì»¬ëŸ¼(ë‚ ì§œ/ì°¸ì—¬BJ/êµ¬ë¶„/í›„ì›í•˜íŠ¸) ë¶€ì¡±ìœ¼ë¡œ ìš”ì•½ì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                piv = (merged.groupby(["ë‚ ì§œ","ì°¸ì—¬BJ","êµ¬ë¶„"], as_index=False)["í›„ì›í•˜íŠ¸"].sum()
                             .pivot(index=["ë‚ ì§œ","ì°¸ì—¬BJ"], columns="êµ¬ë¶„", values="í›„ì›í•˜íŠ¸")
                             .fillna(0).reset_index())
                for col in ["ì¼ë°˜í•˜íŠ¸","ì œíœ´í•˜íŠ¸"]:
                    if col not in piv.columns: piv[col] = 0
                piv["ì´í•©"] = piv["ì¼ë°˜í•˜íŠ¸"] + piv["ì œíœ´í•˜íŠ¸"]
                daily_out = piv[["ë‚ ì§œ","ì°¸ì—¬BJ","ì¼ë°˜í•˜íŠ¸","ì œíœ´í•˜íŠ¸","ì´í•©"]].sort_values(["ë‚ ì§œ","ì°¸ì—¬BJ"]).reset_index(drop=True)
                st.subheader("ìš”ì•½_ì¼ë³„"); st.dataframe(daily_out, use_container_width=True, hide_index=True)

                merged["ì°¸ì—¬BJ_ì •ê·œí™”"] = merged["ì°¸ì—¬BJ"].apply(normalize_bj)
                total_by_bj = (merged.groupby(["ì°¸ì—¬BJ_ì •ê·œí™”","êµ¬ë¶„"], as_index=False)["í›„ì›í•˜íŠ¸"].sum()
                                      .pivot(index="ì°¸ì—¬BJ_ì •ê·œí™”", columns="êµ¬ë¶„", values="í›„ì›í•˜íŠ¸")
                                      .fillna(0).reset_index()
                                      .rename(columns={"ì°¸ì—¬BJ_ì •ê·œí™”":"ì°¸ì—¬BJ"}))
                for col in ["ì¼ë°˜í•˜íŠ¸","ì œíœ´í•˜íŠ¸"]:
                    if col not in total_by_bj.columns: total_by_bj[col] = 0
                total_by_bj["ì´í•©"] = total_by_bj["ì¼ë°˜í•˜íŠ¸"] + total_by_bj["ì œíœ´í•˜íŠ¸"]
                st.subheader("ìš”ì•½_ì°¸ì—¬BJ_ì´ê³„ (ì •ê·œí™” ì ìš©)")
                st.dataframe(total_by_bj.sort_values("ì´í•©", ascending=False), use_container_width=True, hide_index=True)

                master_bytes = build_master_excel_bytes(
                    merged_df=merged,
                    df_daily=daily_out,
                    df_total=total_by_bj[["ì°¸ì—¬BJ","ì¼ë°˜í•˜íŠ¸","ì œíœ´í•˜íŠ¸","ì´í•©"]]
                )
                st.download_button("ğŸ“¥ ì´í•©ì‚° ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                                   data=master_bytes,
                                   file_name="ì´í•©ì‚°.xlsx",
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                   use_container_width=True)
